{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5832b7b0",
   "metadata": {},
   "source": [
    "# Lab 6 ETL Pipeline with streaming tables\n",
    "\n",
    "## Exercise 1\n",
    "\n",
    "1. Create a new ETL pipeline in Databricks and give et a name like Raspdata SDP <your name>\"\n",
    "\n",
    "2. Find the SQL in the Demo folder we used for the ETL pipeline in plenum with the raspdata\n",
    "\n",
    "3. Adjust the query to use a folder with files in your own volume\n",
    "\n",
    "4. Make sure to create silver and bronze schemas for your ETL pipeline before you run it\n",
    "\n",
    "5. Download a couple of the csv files with data from the day2_catalog volume and upload to your folder\n",
    "\n",
    "6. Run your pipeline and see how it goes\n",
    "\n",
    "7. Be inspired here and add some other expectations and constraints to your streaming tables using syntax found here:https://docs.databricks.com/aws/en/ldp/expectations\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
